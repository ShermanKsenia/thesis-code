{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:50:00.567788Z","iopub.status.busy":"2024-05-25T21:50:00.567457Z","iopub.status.idle":"2024-05-25T21:50:13.576233Z","shell.execute_reply":"2024-05-25T21:50:13.575001Z","shell.execute_reply.started":"2024-05-25T21:50:00.567753Z"},"id":"mXypXG-ppj9o","trusted":true},"outputs":[],"source":["!pip install transformers[torch] -q"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:50:13.579024Z","iopub.status.busy":"2024-05-25T21:50:13.578379Z","iopub.status.idle":"2024-05-25T21:50:40.363750Z","shell.execute_reply":"2024-05-25T21:50:40.362668Z","shell.execute_reply.started":"2024-05-25T21:50:13.578986Z"},"id":"wwIXFj7fwjJp","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cubinlinker, which is not installed.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cudf 23.8.0 requires ptxcompiler, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\n","beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.0.11 which is incompatible.\n","cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n","cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n","cudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 16.1.0 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\n","cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\n","dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install datasets -q --upgrade"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:50:40.365691Z","iopub.status.busy":"2024-05-25T21:50:40.365325Z","iopub.status.idle":"2024-05-25T21:50:52.873544Z","shell.execute_reply":"2024-05-25T21:50:52.872450Z","shell.execute_reply.started":"2024-05-25T21:50:40.365655Z"},"trusted":true},"outputs":[],"source":["!pip install evaluate -q"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:50:52.876616Z","iopub.status.busy":"2024-05-25T21:50:52.876313Z","iopub.status.idle":"2024-05-25T21:50:52.880651Z","shell.execute_reply":"2024-05-25T21:50:52.879775Z","shell.execute_reply.started":"2024-05-25T21:50:52.876589Z"},"trusted":true},"outputs":[],"source":["# !pip install wandb -q"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:50:52.881999Z","iopub.status.busy":"2024-05-25T21:50:52.881737Z","iopub.status.idle":"2024-05-25T21:51:13.063789Z","shell.execute_reply":"2024-05-25T21:51:13.062996Z","shell.execute_reply.started":"2024-05-25T21:50:52.881977Z"},"id":"jaSYKqmCs9PP","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-25 21:51:02.630796: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-25 21:51:02.630916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-25 21:51:02.767617: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import os\n","import gc\n","import math\n","import librosa\n","import numpy as np\n","import pandas as pd\n","import librosa as lb\n","import matplotlib.pyplot as plt\n","\n","from scipy import signal\n","from scipy.fft import fftshift\n","\n","from collections import defaultdict\n","from itertools import islice\n","from typing import Any\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","import torch\n","import torchaudio\n","\n","from IPython import display\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from tqdm.auto import trange\n","from tqdm import tqdm\n","tqdm.pandas()\n","\n","import re\n","import seaborn as sns\n","from dataclasses import dataclass, field\n","from typing import Any, Dict, List, Optional, Union\n","from datasets import (\n","    load_dataset,\n","    load_metric,\n","    load_from_disk,\n","    Audio,\n","    concatenate_datasets,\n","    Dataset\n",")\n","from transformers import (\n","    AutoConfig,\n","    Wav2Vec2Processor,\n","    Wav2Vec2Tokenizer,\n","    Wav2Vec2Config,\n","    Wav2Vec2FeatureExtractor,\n","    Wav2Vec2CTCTokenizer,\n","    Wav2Vec2ForSequenceClassification,\n","    Wav2Vec2Model,\n","    TrainingArguments,\n","    Trainer,\n","    get_linear_schedule_with_warmup,\n","    EvalPrediction\n",")\n","\n","from transformers.models.wav2vec2.modeling_wav2vec2 import (\n","    Wav2Vec2PreTrainedModel,\n","    Wav2Vec2Model\n",")\n","import evaluate"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:51:13.580680Z","iopub.status.busy":"2024-05-25T21:51:13.580277Z","iopub.status.idle":"2024-05-25T21:53:06.132320Z","shell.execute_reply":"2024-05-25T21:53:06.131415Z","shell.execute_reply.started":"2024-05-25T21:51:13.580647Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:53:06.134274Z","iopub.status.busy":"2024-05-25T21:53:06.133553Z","iopub.status.idle":"2024-05-25T21:53:06.138779Z","shell.execute_reply":"2024-05-25T21:53:06.137882Z","shell.execute_reply.started":"2024-05-25T21:53:06.134244Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"WANDB_PROJECT\"]=\"thesis_model_batch\"\n","os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:53:06.142389Z","iopub.status.busy":"2024-05-25T21:53:06.142076Z","iopub.status.idle":"2024-05-25T21:53:06.150915Z","shell.execute_reply":"2024-05-25T21:53:06.150168Z","shell.execute_reply.started":"2024-05-25T21:53:06.142365Z"},"id":"-ftrJdwDN9Cp","outputId":"5fe5fb9e-89c0-43bf-afde-ebba4bab29af","trusted":true},"outputs":[{"data":{"text/plain":["'2.19.1'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import datasets\n","datasets.__version__"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:53:06.152121Z","iopub.status.busy":"2024-05-25T21:53:06.151862Z","iopub.status.idle":"2024-05-25T21:53:06.160847Z","shell.execute_reply":"2024-05-25T21:53:06.159992Z","shell.execute_reply.started":"2024-05-25T21:53:06.152084Z"},"id":"_l3dxXdItFTv","trusted":true},"outputs":[],"source":["train_path = '/kaggle/input/thesis-dataset-full/train'\n","train_path_aug1 = '/kaggle/input/thesis-dataset-full/train_augs/train'\n","train_path_aug2 = '/kaggle/input/thesis-dataset-full/train_augs/train 2'\n","val_path = '/kaggle/input/thesis-dataset-full/data_full/val_full.pickle'\n","test_path = '/kaggle/input/thesis-dataset-full/data_full/test_full.pickle'\n","path_to_save = '/kaggle/working/first_model.pth'\n","\n","model_name = 'facebook/wav2vec2-large-xlsr-53'\n","run_name = 'basic_model_one_tahn_russian'"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:53:06.162057Z","iopub.status.busy":"2024-05-25T21:53:06.161810Z","iopub.status.idle":"2024-05-25T21:53:29.807944Z","shell.execute_reply":"2024-05-25T21:53:29.807070Z","shell.execute_reply.started":"2024-05-25T21:53:06.162035Z"},"id":"e16LzUvWxJwS","outputId":"f4d0f7e2-e68c-4058-9d4a-10e1fb16bae8","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f72974a7eb54adb9190dd6eabeb60be","version_major":2,"version_minor":0},"text/plain":["Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0816a6e58a92440ba1e40b1b9388823e","version_major":2,"version_minor":0},"text/plain":["Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train1 = load_from_disk(train_path_aug1)\n","train2 = load_from_disk(train_path_aug2)\n","\n","val_pd = pd.read_pickle(val_path)\n","test_pd = pd.read_pickle(test_path)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:53:29.809313Z","iopub.status.busy":"2024-05-25T21:53:29.808992Z","iopub.status.idle":"2024-05-25T21:53:29.877124Z","shell.execute_reply":"2024-05-25T21:53:29.876232Z","shell.execute_reply.started":"2024-05-25T21:53:29.809286Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(19878, 14812, 17495, 13948)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import concatenate_datasets\n","\n","train = concatenate_datasets([train1, train2])\n","labels = train['labels']\n","labels.count(0), labels.count(1), labels.count(2), labels.count(3)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:53:29.916250Z","iopub.status.busy":"2024-05-25T21:53:29.915970Z","iopub.status.idle":"2024-05-25T21:53:29.923213Z","shell.execute_reply":"2024-05-25T21:53:29.922368Z","shell.execute_reply.started":"2024-05-25T21:53:29.916227Z"},"id":"_nNHoRpExzMy","trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, input_values, labels, audio_paths):\n","        super().__init__()\n","        self.input_values = input_values\n","        self.labels = labels\n","        self.audio_paths = audio_paths\n","\n","    def __len__(self):\n","        return len(self.input_values)\n","\n","    def __getitem__(self, idx):\n","        return {'input_values': self.input_values[idx].tolist(), 'labels': self.labels[idx], 'audio_ID': self.audio_paths[idx]}"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:53:29.924392Z","iopub.status.busy":"2024-05-25T21:53:29.924142Z","iopub.status.idle":"2024-05-25T21:54:00.250627Z","shell.execute_reply":"2024-05-25T21:54:00.249588Z","shell.execute_reply.started":"2024-05-25T21:53:29.924367Z"},"id":"15NvsGx0y2ZX","trusted":true},"outputs":[],"source":["val = Dataset.from_pandas(val_pd)\n","test = Dataset.from_pandas(test_pd)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:00.264332Z","iopub.status.busy":"2024-05-25T21:54:00.263667Z","iopub.status.idle":"2024-05-25T21:54:00.612758Z","shell.execute_reply":"2024-05-25T21:54:00.611775Z","shell.execute_reply.started":"2024-05-25T21:54:00.264301Z"},"id":"TgAK6KgWzR22","trusted":true},"outputs":[{"data":{"text/plain":["80"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del val_pd\n","gc.collect()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:00.614346Z","iopub.status.busy":"2024-05-25T21:54:00.613974Z","iopub.status.idle":"2024-05-25T21:54:00.622567Z","shell.execute_reply":"2024-05-25T21:54:00.621637Z","shell.execute_reply.started":"2024-05-25T21:54:00.614320Z"},"trusted":true},"outputs":[],"source":["from torch import Tensor\n","\n","def collate_fn_pooled_tokens(data):\n","    input_ids = [torch.tensor(data[i]['input_values']).cuda() for i in range(len(data))]\n","    attention_mask = [torch.where(input_ids[i]==0, 0, 1).cuda() for i in range(len(input_ids))]\n","    audio_ids = [data[i]['audio_ID'] for i in range(len(data))]\n","    if len(data[0]) == 2:\n","        collated = [input_ids, attention_mask, audio_ids]\n","        # collated = [input_ids]\n","    else:\n","        labels = Tensor([data[i]['labels'] for i in range(len(data))])\n","        collated = [input_ids, attention_mask, labels, audio_ids]\n","        # collated = [input_ids, labels]\n","    return collated"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:00.637521Z","iopub.status.busy":"2024-05-25T21:54:00.637200Z","iopub.status.idle":"2024-05-25T21:54:00.644836Z","shell.execute_reply":"2024-05-25T21:54:00.644090Z","shell.execute_reply.started":"2024-05-25T21:54:00.637491Z"},"id":"1BKpySkJz6YA","trusted":true},"outputs":[],"source":["train_dataloader_splitted = DataLoader(train, batch_size=8, collate_fn=collate_fn_pooled_tokens)\n","val_dataloader_splitted = DataLoader(val, batch_size=8, collate_fn=collate_fn_pooled_tokens)"]},{"cell_type":"markdown","metadata":{"id":"VmaIRf_i8h86"},"source":["## Initialize model"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:01.707297Z","iopub.status.busy":"2024-05-25T21:54:01.706931Z","iopub.status.idle":"2024-05-25T21:54:01.714081Z","shell.execute_reply":"2024-05-25T21:54:01.713308Z","shell.execute_reply.started":"2024-05-25T21:54:01.707261Z"},"id":"4-MphDgXDuEz","trusted":true},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import Optional, Tuple\n","import torch\n","from transformers.file_utils import ModelOutput\n","\n","\n","@dataclass\n","class SpeechClassifierOutput(ModelOutput):\n","    loss: Optional[torch.FloatTensor] = None\n","    logits: torch.FloatTensor = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:01.730579Z","iopub.status.busy":"2024-05-25T21:54:01.730072Z","iopub.status.idle":"2024-05-25T21:54:01.751137Z","shell.execute_reply":"2024-05-25T21:54:01.750277Z","shell.execute_reply.started":"2024-05-25T21:54:01.730554Z"},"trusted":true},"outputs":[],"source":["# https://github.com/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb\n","\n","class Wav2Vec2ClassificationHead(nn.Module):\n","    \"\"\"Head for wav2vec classification task.\"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(1024, config['hidden_size'])\n","        self.hidden_dropout = nn.Dropout(config['hidden_dropout'])\n","        self.final_dropout = nn.Dropout(config['final_dropout'])\n","        self.out_proj = nn.Linear(config['hidden_size'], config['num_labels'])\n","        self.sigmoid = nn.Sigmoid()\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax()\n","\n","    def forward(self, features, **kwargs):\n","        x = self.hidden_dropout(features)\n","        x = self.dense(x)\n","        x = self.relu(x)\n","        x = self.final_dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","\n","class Wav2VecClassifer(Wav2Vec2PreTrainedModel):\n","\n","    def __init__(self, w2v_config, config):\n","        super().__init__(w2v_config)\n","\n","        self.num_labels = config['num_labels']\n","        self.pooling_strategy = config['pooling_mode']\n","\n","        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\n","            config['model_name'], \n","            config=w2v_config)\n","\n","        self.classifier = Wav2Vec2ClassificationHead(config)#.to(self.device)\n","        self.loss = nn.CrossEntropyLoss()\n","\n","        self.post_init()\n","\n","    def freeze_feature_extractor(self):\n","        self.wav2vec2.feature_extractor._freeze_parameters()\n","\n","    def merged_strategy(\n","            self,\n","            hidden_states,\n","            mode=\"mean\"\n","    ):\n","        if mode == \"mean\":\n","            outputs = torch.mean(hidden_states, dim=1)\n","        elif mode == \"sum\":\n","            outputs = torch.sum(hidden_states, dim=1)\n","        elif mode == \"max\":\n","            outputs = torch.max(hidden_states, dim=1)[0]\n","        else:\n","            raise Exception(\n","                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n","\n","        return outputs\n","\n","    def forward(\n","            self,\n","            input_values,\n","            labels,\n","            attention_mask=None,\n","            device=None,\n","            output_attentions=False,\n","            output_hidden_states=False,\n","            number_of_chunks=None\n","    ):\n","\n","        outputs = self.wav2vec2(\n","            input_values=input_values,\n","            attention_mask=attention_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states\n","        )\n","        \n","        hidden_states = outputs[0]\n","\n","        if attention_mask is None:\n","            pooled_output = hidden_states.mean(dim=1)\n","        else:\n","            padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n","            padding_mask = padding_mask\n","            hidden_states[~padding_mask] = -100\n","            pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n","\n","        logits = self.classifier(pooled_output)\n","        preds = logits.view(-1, self.num_labels)\n","        loss = None\n","        if labels is not None:\n","            loss = self.loss(preds, labels.view(-1)) #\n","        \n","        output = (preds,) + outputs[2:]\n","        return ((loss,) + output) if loss is not None else output"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:01.752545Z","iopub.status.busy":"2024-05-25T21:54:01.752246Z","iopub.status.idle":"2024-05-25T21:54:06.623287Z","shell.execute_reply":"2024-05-25T21:54:06.622277Z","shell.execute_reply.started":"2024-05-25T21:54:01.752521Z"},"id":"Gk_FY8Rd_SNx","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c6c72b277a3411c81816ec35535c860","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.78k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68b600c3060847279b8fe25cd1c86bf3","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-russian and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["config = {\n","    'output_hidden_states': False,\n","    'num_labels': 2,\n","    'hidden_size': 64,\n","    'final_dropout': 0.1,\n","    'hidden_dropout': 0.2,\n","    'pooling_mode': 'mean',\n","    'model_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian',\n","    'input_size': 1024\n","}\n","\n","model_args_w2v2 = {\n","    \"attention_dropout\": 0.1,\n","    \"hidden_dropout\": 0.1,\n","    \"feat_proj_dropout\": 0.0,\n","    \"mask_time_prob\": 0.05,\n","    \"layerdrop\": 0.1,\n","    'layer_norm_eps':1e-4,\n","}\n","\n","model_args_w2v2_config = Wav2Vec2Config.from_pretrained(\n","    config['model_name'], \n","    attention_dropout=0.1,\n","    hidden_dropout=0.1,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.1,\n",")\n","\n","model = Wav2VecClassifer(model_args_w2v2_config, config).cuda()"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:06.624833Z","iopub.status.busy":"2024-05-25T21:54:06.624539Z","iopub.status.idle":"2024-05-25T21:54:06.629746Z","shell.execute_reply":"2024-05-25T21:54:06.628754Z","shell.execute_reply.started":"2024-05-25T21:54:06.624808Z"},"trusted":true},"outputs":[],"source":["model.freeze_feature_extractor()"]},{"cell_type":"markdown","metadata":{},"source":["### Trainer"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:06.631429Z","iopub.status.busy":"2024-05-25T21:54:06.631021Z","iopub.status.idle":"2024-05-25T21:54:06.644916Z","shell.execute_reply":"2024-05-25T21:54:06.644157Z","shell.execute_reply.started":"2024-05-25T21:54:06.631395Z"},"trusted":true},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import Dict, List, Optional, Union\n","import torch\n","\n","import transformers\n","from transformers import Wav2Vec2Processor\n","\n","\n","@dataclass\n","class DataCollatorCTCWithPadding:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs received.\n","    Args:\n","        processor (:class:`~transformers.Wav2Vec2Processor`)\n","            The processor used for proccessing the data.\n","        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n","              sequence if provided).\n","            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n","              maximum acceptable input length for the model if that argument is not provided.\n","            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n","              different lengths).\n","        max_length (:obj:`int`, `optional`):\n","            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n","        max_length_labels (:obj:`int`, `optional`):\n","            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n","        pad_to_multiple_of (:obj:`int`, `optional`):\n","            If set will pad the sequence to a multiple of the provided value.\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","    \"\"\"\n","\n","    processor: Wav2Vec2Processor\n","    padding: Union[bool, str] = True\n","    max_length: Optional[int] = None\n","    max_length_labels: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    pad_to_multiple_of_labels: Optional[int] = None\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n","        label_features = [feature[\"labels\"] for feature in features]\n","\n","        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n","\n","        batch = self.processor.pad(\n","            input_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","\n","        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n","#         print(batch)\n","\n","        return batch"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:06.646315Z","iopub.status.busy":"2024-05-25T21:54:06.645967Z","iopub.status.idle":"2024-05-25T21:54:06.841393Z","shell.execute_reply":"2024-05-25T21:54:06.840572Z","shell.execute_reply.started":"2024-05-25T21:54:06.646284Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4bc82d4430ad4846a13b646d7bc316b0","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/262 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["processor = Wav2Vec2FeatureExtractor.from_pretrained('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\n","data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:06.842900Z","iopub.status.busy":"2024-05-25T21:54:06.842576Z","iopub.status.idle":"2024-05-25T21:54:06.851123Z","shell.execute_reply":"2024-05-25T21:54:06.850184Z","shell.execute_reply.started":"2024-05-25T21:54:06.842869Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","    labels = p.label_ids\n","    \n","    cr = evaluate.load('bstrai/classification_report')\n","    result = cr.compute(predictions=preds, references=labels, labels=[0, 1, 2, 3])\n","\n","    res_dict = {}\n","    for label in result.keys():\n","        if isinstance(result[label], dict):\n","            for metric in result[label].keys():\n","                res_dict[f'{metric}_{label}'] = result[label][metric]\n","        else:\n","            res_dict[label] = result[label]\n","\n","    return res_dict"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:06.852513Z","iopub.status.busy":"2024-05-25T21:54:06.852278Z","iopub.status.idle":"2024-05-25T21:54:06.864279Z","shell.execute_reply":"2024-05-25T21:54:06.863427Z","shell.execute_reply.started":"2024-05-25T21:54:06.852492Z"},"trusted":true},"outputs":[],"source":["from transformers import TrainingArguments\n","import random\n","\n","def set_seed(seed: int, deterministic: bool = False):\n","    \"\"\"\n","    Helper function for reproducible behavior to set the seed in `random`, `numpy`, `torch` and/or `tf` (if installed).\n","\n","    Args:\n","        seed (`int`):\n","            The seed to set.\n","        deterministic (`bool`, *optional*, defaults to `False`):\n","            Whether to use deterministic algorithms where available. Can slow down training.\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    # ^^ safe to call this function even if cuda is not available\n","    if deterministic:\n","        torch.use_deterministic_algorithms(True)\n","        \n","set_seed(888) #888"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:06.865710Z","iopub.status.busy":"2024-05-25T21:54:06.865386Z","iopub.status.idle":"2024-05-25T21:54:06.877395Z","shell.execute_reply":"2024-05-25T21:54:06.876602Z","shell.execute_reply.started":"2024-05-25T21:54:06.865685Z"},"trusted":true},"outputs":[],"source":["num_epochs = 2\n","warmup_ratio = 0.1\n","total_steps = len(train_dataloader_splitted) * num_epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T21:54:06.890529Z","iopub.status.busy":"2024-05-25T21:54:06.890067Z"},"trusted":true},"outputs":[],"source":["# for lr in [1e-3, 2e-3, 1e-4]\n","lr = 1e-3\n","config = {\n","    'output_hidden_states': False,\n","    'num_labels': 4,\n","    'hidden_size': 64,\n","    'final_dropout': 0.1,\n","    'hidden_dropout': 0.2,\n","    'pooling_mode': 'mean',\n","    'model_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian',\n","    'input_size': 1024\n","}\n","\n","model_args_w2v2 = {\n","    \"attention_dropout\": 0.1,\n","    \"hidden_dropout\": 0.1,\n","    \"feat_proj_dropout\": 0.0,\n","    \"mask_time_prob\": 0.05,\n","    \"layerdrop\": 0.1,\n","    'layer_norm_eps':1e-4,\n","}\n","\n","model_args_w2v2_config = Wav2Vec2Config.from_pretrained(\n","    config['model_name'], \n","    attention_dropout=0.1,\n","    hidden_dropout=0.1,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.1,\n",")\n","\n","model = Wav2VecClassifer(model_args_w2v2_config, config).cuda()\n","model.freeze_feature_extractor()\n","\n","optimizer = torch.optim.AdamW([\n","        {\"params\": model.wav2vec2.parameters(), \"lr\": 3e-4},\n","    ], lr=lr)\n","\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=total_steps * warmup_ratio,\n","    num_training_steps=total_steps\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"/kaggle/working/model_trainer_{lr}\",\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    evaluation_strategy=\"steps\",\n","    num_train_epochs=num_epochs,\n","    fp16=True,\n","    save_steps=3000,\n","    eval_steps=1500,\n","    learning_rate=lr,\n","    save_total_limit=3,\n","    report_to='wandb',\n","    run_name=f\"model_2_{lr}_w2v_3e-4_one_value_full_augs_888\",\n","    logging_steps=1000,\n","    load_best_model_at_end=True,\n","    seed=888\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train,\n","    eval_dataset=val,\n","    tokenizer=processor,\n","    optimizers=(optimizer, scheduler)\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T11:27:10.975886Z","iopub.status.busy":"2024-05-26T11:27:10.975573Z","iopub.status.idle":"2024-05-26T11:27:11.299641Z","shell.execute_reply":"2024-05-26T11:27:11.298489Z","shell.execute_reply.started":"2024-05-26T11:27:10.975860Z"},"trusted":true},"outputs":[],"source":["wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4993776,"sourceId":8514892,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0de778fff8a64cb089e3ae5bad4ca3e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25d14ebc364145e9a635c394d4f69d33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33da96d697b0497da57d6c51dac2b573","placeholder":"​","style":"IPY_MODEL_e802144479d84603b99e3d6c718cc21a","value":"Loading dataset from disk: 100%"}},"33da96d697b0497da57d6c51dac2b573":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44c1eafe3ea248aa9360f44c6d808d78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4aaba4e709844edaa8926ffe5696675f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"623d9824a7544de8b050730f4ebac305":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"665e535568a1464dbdb76069bf14375f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d98cae5ea863464299cc52aa58f96bf0","placeholder":"​","style":"IPY_MODEL_0de778fff8a64cb089e3ae5bad4ca3e6","value":" 24/24 [00:00&lt;00:00, 53.01it/s]"}},"912ca1284c2c4bfd88b86e13f70394d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_25d14ebc364145e9a635c394d4f69d33","IPY_MODEL_c91d6f89bae849d6a6b6970ccdcd89c3","IPY_MODEL_665e535568a1464dbdb76069bf14375f"],"layout":"IPY_MODEL_4aaba4e709844edaa8926ffe5696675f"}},"c91d6f89bae849d6a6b6970ccdcd89c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_623d9824a7544de8b050730f4ebac305","max":24,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44c1eafe3ea248aa9360f44c6d808d78","value":24}},"d98cae5ea863464299cc52aa58f96bf0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e802144479d84603b99e3d6c718cc21a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
