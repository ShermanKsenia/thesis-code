{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:55:09.397342Z","iopub.status.busy":"2024-05-26T20:55:09.397013Z","iopub.status.idle":"2024-05-26T20:55:23.288541Z","shell.execute_reply":"2024-05-26T20:55:23.287357Z","shell.execute_reply.started":"2024-05-26T20:55:09.397316Z"},"id":"mXypXG-ppj9o","trusted":true},"outputs":[],"source":["!pip install transformers[torch] -q"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:55:23.290672Z","iopub.status.busy":"2024-05-26T20:55:23.290405Z","iopub.status.idle":"2024-05-26T20:55:36.494718Z","shell.execute_reply":"2024-05-26T20:55:36.493673Z","shell.execute_reply.started":"2024-05-26T20:55:23.290649Z"},"id":"wwIXFj7fwjJp","trusted":true},"outputs":[],"source":["!pip install datasets -q --upgrade"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:55:36.496309Z","iopub.status.busy":"2024-05-26T20:55:36.496036Z","iopub.status.idle":"2024-05-26T20:55:49.240198Z","shell.execute_reply":"2024-05-26T20:55:49.239003Z","shell.execute_reply.started":"2024-05-26T20:55:36.496284Z"},"trusted":true},"outputs":[],"source":["!pip install evaluate -q"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:55:49.243054Z","iopub.status.busy":"2024-05-26T20:55:49.242702Z","iopub.status.idle":"2024-05-26T20:56:05.036410Z","shell.execute_reply":"2024-05-26T20:56:05.035358Z","shell.execute_reply.started":"2024-05-26T20:55:49.243025Z"},"trusted":true},"outputs":[],"source":["!pip install wandb -q --upgrade"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:56:05.038014Z","iopub.status.busy":"2024-05-26T20:56:05.037711Z","iopub.status.idle":"2024-05-26T20:56:22.759974Z","shell.execute_reply":"2024-05-26T20:56:22.759183Z","shell.execute_reply.started":"2024-05-26T20:56:05.037987Z"},"id":"jaSYKqmCs9PP","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-26 20:56:14.868154: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-26 20:56:14.868249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-26 20:56:14.996242: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import os\n","import gc\n","import math\n","import librosa\n","import numpy as np\n","import pandas as pd\n","import librosa as lb\n","import matplotlib.pyplot as plt\n","\n","from scipy import signal\n","from scipy.fft import fftshift\n","\n","from collections import defaultdict\n","from itertools import islice\n","from typing import Any\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","import torch\n","import torchaudio\n","\n","from IPython import display\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from tqdm.auto import trange\n","from tqdm import tqdm\n","tqdm.pandas()\n","\n","import re\n","import seaborn as sns\n","from dataclasses import dataclass, field\n","from typing import Any, Dict, List, Optional, Union\n","from datasets import (\n","    load_dataset,\n","    load_metric,\n","    load_from_disk,\n","    Audio,\n","    concatenate_datasets\n",")\n","from transformers import (\n","    AutoConfig,\n","    Wav2Vec2Processor,\n","    Wav2Vec2Tokenizer,\n","    Wav2Vec2Config,\n","    Wav2Vec2FeatureExtractor,\n","    Wav2Vec2CTCTokenizer,\n","    Wav2Vec2ForSequenceClassification,\n","    Wav2Vec2Model,\n","    TrainingArguments,\n","    Trainer,\n","    get_linear_schedule_with_warmup,\n","    EvalPrediction\n",")\n","\n","from transformers.models.wav2vec2.modeling_wav2vec2 import (\n","    Wav2Vec2PreTrainedModel,\n","    Wav2Vec2Model\n",")\n","import evaluate"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:56:22.761833Z","iopub.status.busy":"2024-05-26T20:56:22.761137Z","iopub.status.idle":"2024-05-26T20:57:22.293990Z","shell.execute_reply":"2024-05-26T20:57:22.292981Z","shell.execute_reply.started":"2024-05-26T20:56:22.761807Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:57:22.298518Z","iopub.status.busy":"2024-05-26T20:57:22.297618Z","iopub.status.idle":"2024-05-26T20:57:22.303453Z","shell.execute_reply":"2024-05-26T20:57:22.302442Z","shell.execute_reply.started":"2024-05-26T20:57:22.298483Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"WANDB_PROJECT\"]=\"thesis_model_batch\"\n","os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:57:22.304818Z","iopub.status.busy":"2024-05-26T20:57:22.304514Z","iopub.status.idle":"2024-05-26T20:57:22.317301Z","shell.execute_reply":"2024-05-26T20:57:22.316229Z","shell.execute_reply.started":"2024-05-26T20:57:22.304792Z"},"id":"-ftrJdwDN9Cp","outputId":"5fe5fb9e-89c0-43bf-afde-ebba4bab29af","trusted":true},"outputs":[{"data":{"text/plain":["'2.19.1'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import datasets\n","datasets.__version__"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:57:22.319332Z","iopub.status.busy":"2024-05-26T20:57:22.319082Z","iopub.status.idle":"2024-05-26T20:57:22.327454Z","shell.execute_reply":"2024-05-26T20:57:22.326586Z","shell.execute_reply.started":"2024-05-26T20:57:22.319311Z"},"id":"_l3dxXdItFTv","trusted":true},"outputs":[],"source":["train_path = '/kaggle/input/thesis-train-data/train_one'\n","\n","train1_path = '/kaggle/input/thesis-model-batch-3sec-8/train1'\n","train2_path = '/kaggle/input/thesis-model-batch-3sec-8/train2'\n","\n","val_path = '/kaggle/input/thesis-train-data/val_one.pickle'\n","path_to_save = '/kaggle/working/second_model.pth'\n","\n","model_name = 'facebook/wa|v2vec2-large-xlsr-53'\n","run_name = 'basic_model_one_tahn_russian'"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:57:22.332233Z","iopub.status.busy":"2024-05-26T20:57:22.331903Z","iopub.status.idle":"2024-05-26T20:57:33.077210Z","shell.execute_reply":"2024-05-26T20:57:33.076350Z","shell.execute_reply.started":"2024-05-26T20:57:22.332201Z"},"id":"e16LzUvWxJwS","outputId":"f4d0f7e2-e68c-4058-9d4a-10e1fb16bae8","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0b1399436cb4514ac56e7be1bc3a666","version_major":2,"version_minor":0},"text/plain":["Loading dataset from disk:   0%|          | 0/24 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import concatenate_datasets\n","train1 = load_from_disk(train1_path)\n","train2 = load_from_disk(train2_path)\n","\n","train = concatenate_datasets([train1, train2])\n","val_pd = pd.read_pickle(val_path)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:57:33.115192Z","iopub.status.busy":"2024-05-26T20:57:33.114812Z","iopub.status.idle":"2024-05-26T20:57:33.123128Z","shell.execute_reply":"2024-05-26T20:57:33.122229Z","shell.execute_reply.started":"2024-05-26T20:57:33.115162Z"},"id":"_nNHoRpExzMy","trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, input_values, labels, audio_paths):\n","        super().__init__()\n","        self.input_values = input_values\n","        self.labels = labels\n","        self.audio_paths = audio_paths\n","\n","    def __len__(self):\n","        return len(self.input_values)\n","\n","    def __getitem__(self, idx):\n","        return {'input_values': self.input_values[idx].tolist(), 'labels': self.labels[idx], 'audio_ID': self.audio_paths[idx]}"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:57:33.124553Z","iopub.status.busy":"2024-05-26T20:57:33.124284Z","iopub.status.idle":"2024-05-26T20:57:33.133352Z","shell.execute_reply":"2024-05-26T20:57:33.132463Z","shell.execute_reply.started":"2024-05-26T20:57:33.124531Z"},"id":"15NvsGx0y2ZX","trusted":true},"outputs":[],"source":["val = CustomDataset(\n","    input_values = val_pd['input_values'].tolist(),\n","    labels = val_pd['labels'].tolist(),\n","    audio_paths = val_pd['audio_ID'].tolist()\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:57:33.134557Z","iopub.status.busy":"2024-05-26T20:57:33.134270Z","iopub.status.idle":"2024-05-26T20:57:33.451404Z","shell.execute_reply":"2024-05-26T20:57:33.450396Z","shell.execute_reply.started":"2024-05-26T20:57:33.134535Z"},"id":"TgAK6KgWzR22","trusted":true},"outputs":[{"data":{"text/plain":["1259"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del val_pd\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"VmaIRf_i8h86"},"source":["## Initialize model"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:57:33.508378Z","iopub.status.busy":"2024-05-26T20:57:33.508121Z","iopub.status.idle":"2024-05-26T20:57:33.518682Z","shell.execute_reply":"2024-05-26T20:57:33.517836Z","shell.execute_reply.started":"2024-05-26T20:57:33.508357Z"},"id":"4-MphDgXDuEz","trusted":true},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import Optional, Tuple\n","import torch\n","from transformers.file_utils import ModelOutput\n","\n","\n","@dataclass\n","class SpeechClassifierOutput(ModelOutput):\n","    loss: Optional[torch.FloatTensor] = None\n","    logits: torch.FloatTensor = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:59:02.799483Z","iopub.status.busy":"2024-05-26T20:59:02.798720Z","iopub.status.idle":"2024-05-26T20:59:02.821713Z","shell.execute_reply":"2024-05-26T20:59:02.820601Z","shell.execute_reply.started":"2024-05-26T20:59:02.799452Z"},"trusted":true},"outputs":[],"source":["# https://github.com/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb\n","import torch\n","from torch import nn\n","from torch.nn import functional as nnf\n","from tqdm import tqdm\n","tqdm.pandas()\n","\n","from transformers import (\n","    Wav2Vec2Model\n",")\n","from transformers.models.wav2vec2.modeling_wav2vec2 import (\n","    Wav2Vec2PreTrainedModel,\n","    Wav2Vec2Model\n",")\n","\n","class Wav2Vec2ClassificationHead(nn.Module):\n","    \"\"\"Head for wav2vec classification task.\"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.lstm = nn.LSTM(\n","            config['input_size'],\n","            config['hidden_size'],\n","            num_layers = 2,\n","            bidirectional = True,\n","            dropout = 0.1,\n","            batch_first = True\n","        )\n","        \n","        self.fc = nn.Linear(config['hidden_size'] * 2,config['num_labels'])\n","    \n","    def forward(self,input_values):     \n","        packed_output,(hidden_state,cell_state) = self.lstm(input_values)\n","        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1) \n","        dense_outputs=self.fc(hidden)\n","        return dense_outputs\n","\n","class Wav2VecClassifer(Wav2Vec2PreTrainedModel):\n","\n","    def __init__(self, w2v_config, config):\n","        super().__init__(w2v_config)\n","\n","        self.num_labels = config['num_labels']\n","        self.pooling_strategy = config['pooling_mode']\n","        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\n","            config['model_name'], \n","            config=w2v_config\n","        )\n","        self.proj_layer = nn.Linear(config['input_size'], 256)\n","        self.classifier = Wav2Vec2ClassificationHead(config)\n","        self.loss = nn.CrossEntropyLoss()\n","        self.post_init()\n","\n","    def freeze_feature_extractor(self):\n","        self.wav2vec2.feature_extractor._freeze_parameters()\n","\n","    def merged_strategy(\n","            self,\n","            hidden_states,\n","            mode=\"mean\"\n","    ):\n","        if mode == \"mean\":\n","            outputs = torch.mean(hidden_states, dim=1)\n","        elif mode == \"sum\":\n","            outputs = torch.sum(hidden_states, dim=1)\n","        elif mode == \"max\":\n","            outputs = torch.max(hidden_states, dim=1)[0]\n","        else:\n","            raise Exception(\n","                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n","\n","        return outputs\n","\n","    def forward(\n","            self,\n","            input_values,\n","            labels,\n","            attention_mask=None,\n","            device=None,\n","            output_attentions=False,\n","            output_hidden_states=False,\n","            number_of_chunks=None\n","    ):\n","\n","#         number_of_chunks = [len(x) for x in input_values]\n","\n","        outputs = self.wav2vec2(\n","            input_values=input_values,\n","            attention_mask=attention_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states\n","        )\n","        hidden_states = outputs.last_hidden_state\n","        all_hs = []\n","        for hs in hidden_states:\n","            all_hs.append(hs.view(1, -1, 1))\n","        chunks_num = len(all_hs) - 1\n","\n","        concat_states = torch.cat(all_hs, dim=2)\n","        raw = nnf.fold(concat_states, (249+100*chunks_num, 1024), kernel_size=(249, 1024), stride=(100, 1024))\n","        counter = nnf.fold(torch.ones_like(concat_states), (249+100*chunks_num, 1024), kernel_size=(249, 1024), stride=(100, 1024))\n","        result = raw / counter\n","\n","        pooled_preds = self.classifier(result.squeeze(0))\n","\n","        loss = None\n","        if labels is not None:\n","            loss = self.loss(pooled_preds.view(-1, 2), labels.view(-1))\n","        output = (pooled_preds,) + outputs[2:]\n","        return ((loss,) + output) if loss is not None else output"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:59:04.423420Z","iopub.status.busy":"2024-05-26T20:59:04.422760Z","iopub.status.idle":"2024-05-26T20:59:05.487381Z","shell.execute_reply":"2024-05-26T20:59:05.486369Z","shell.execute_reply.started":"2024-05-26T20:59:04.423390Z"},"id":"Gk_FY8Rd_SNx","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of Wav2Vec2Model were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-russian and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["config = {\n","    'output_hidden_states': False,\n","    'num_labels': 2,\n","    'hidden_size': 64,\n","    'final_dropout': 0.1,\n","    'hidden_dropout': 0.2,\n","    'pooling_mode': 'mean',\n","    'model_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian',\n","    'input_size': 1024\n","}\n","\n","model_args_w2v2 = {\n","    \"attention_dropout\": 0.1,\n","    \"hidden_dropout\": 0.1,\n","    \"feat_proj_dropout\": 0.0,\n","    \"mask_time_prob\": 0.05,\n","    \"layerdrop\": 0.1,\n","    'layer_norm_eps':1e-4,\n","}\n","\n","model_args_w2v2_config = Wav2Vec2Config.from_pretrained(\n","    config['model_name'], \n","    attention_dropout=0.1,\n","    hidden_dropout=0.1,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.1,\n","#     layer_norm_eps:1e-4,\n",")\n","\n","model = Wav2VecClassifer(model_args_w2v2_config, config).cuda()"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:59:05.490641Z","iopub.status.busy":"2024-05-26T20:59:05.490265Z","iopub.status.idle":"2024-05-26T20:59:05.496761Z","shell.execute_reply":"2024-05-26T20:59:05.495426Z","shell.execute_reply.started":"2024-05-26T20:59:05.490607Z"},"trusted":true},"outputs":[],"source":["model.freeze_feature_extractor()"]},{"cell_type":"markdown","metadata":{},"source":["### Trainer"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:59:05.515438Z","iopub.status.busy":"2024-05-26T20:59:05.515099Z","iopub.status.idle":"2024-05-26T20:59:05.529071Z","shell.execute_reply":"2024-05-26T20:59:05.528044Z","shell.execute_reply.started":"2024-05-26T20:59:05.515407Z"},"trusted":true},"outputs":[],"source":["from torch import Tensor\n","\n","def collate_fn_pooled_tokens(data):\n","    input_ids = [torch.tensor(data[i]['input_values']) for i in range(len(data))]\n","    attention_mask = [torch.where(input_ids[i]==0, 0, 1) for i in range(len(input_ids))]\n","    label_features = [data[i]['labels'] for i in range(len(data))]\n","    d_type = torch.long if isinstance(label_features[0], int) else torch.float\n","    if len(data[0]) == 2:\n","        collated = {\n","            'input_values': input_ids[0],\n","            'attention_mask': attention_mask[0],\n","            'labels': torch.tensor(label_features, dtype=d_type)\n","        }\n","    else:\n","        collated = {\n","            'input_values': input_ids[0],\n","            'attention_mask': attention_mask[0],\n","            'labels': torch.tensor(label_features, dtype=d_type)\n","        }\n","    return collated"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:59:06.524415Z","iopub.status.busy":"2024-05-26T20:59:06.523746Z","iopub.status.idle":"2024-05-26T20:59:06.615732Z","shell.execute_reply":"2024-05-26T20:59:06.614671Z","shell.execute_reply.started":"2024-05-26T20:59:06.524384Z"},"trusted":true},"outputs":[],"source":["processor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-large-xlsr-53')\n","data_collator = collate_fn_pooled_tokens"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:59:06.852239Z","iopub.status.busy":"2024-05-26T20:59:06.851939Z","iopub.status.idle":"2024-05-26T20:59:06.860823Z","shell.execute_reply":"2024-05-26T20:59:06.859713Z","shell.execute_reply.started":"2024-05-26T20:59:06.852214Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","    labels = p.label_ids\n","\n","    cr = evaluate.load('bstrai/classification_report')\n","    result = cr.compute(predictions=preds, references=labels, labels=[0, 1])\n","    \n","    res_dict = {}\n","    for label in result.keys():\n","        if isinstance(result[label], dict):\n","            for metric in result[label].keys():\n","                res_dict[f'{metric}_{label}'] = result[label][metric]\n","        else:\n","            res_dict[label] = result[label]\n","\n","    return res_dict"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:59:07.914289Z","iopub.status.busy":"2024-05-26T20:59:07.913911Z","iopub.status.idle":"2024-05-26T20:59:07.922088Z","shell.execute_reply":"2024-05-26T20:59:07.920989Z","shell.execute_reply.started":"2024-05-26T20:59:07.914264Z"},"trusted":true},"outputs":[],"source":["import random\n","def set_seed(seed: int, deterministic: bool = False):\n","        \"\"\"\n","        Helper function for reproducible behavior to set the seed in `random`, `numpy`, `torch` and/or `tf` (if installed).\n","\n","        Args:\n","            seed (`int`):\n","                The seed to set.\n","            deterministic (`bool`, *optional*, defaults to `False`):\n","                Whether to use deterministic algorithms where available. Can slow down training.\n","        \"\"\"\n","        random.seed(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        # ^^ safe to call this function even if cuda is not available\n","        if deterministic:\n","            torch.use_deterministic_algorithms(True)\n","\n","set_seed(88) #888"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:59:08.558936Z","iopub.status.busy":"2024-05-26T20:59:08.558560Z","iopub.status.idle":"2024-05-26T20:59:08.565112Z","shell.execute_reply":"2024-05-26T20:59:08.564097Z","shell.execute_reply.started":"2024-05-26T20:59:08.558888Z"},"trusted":true},"outputs":[],"source":["from transformers import TrainingArguments"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:59:08.770672Z","iopub.status.busy":"2024-05-26T20:59:08.770357Z","iopub.status.idle":"2024-05-26T20:59:08.776428Z","shell.execute_reply":"2024-05-26T20:59:08.775341Z","shell.execute_reply.started":"2024-05-26T20:59:08.770646Z"},"trusted":true},"outputs":[],"source":["num_epochs = 2\n","warmup_ratio = 0.1\n","total_steps = len(train) * num_epochs"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:59:10.122923Z","iopub.status.busy":"2024-05-26T20:59:10.122101Z","iopub.status.idle":"2024-05-26T20:59:10.130940Z","shell.execute_reply":"2024-05-26T20:59:10.129977Z","shell.execute_reply.started":"2024-05-26T20:59:10.122885Z"},"trusted":true},"outputs":[{"data":{"text/plain":["74556"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["total_steps"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T20:59:17.445477Z","iopub.status.busy":"2024-05-26T20:59:17.445139Z","iopub.status.idle":"2024-05-26T20:59:21.251054Z","shell.execute_reply":"2024-05-26T20:59:21.249220Z","shell.execute_reply.started":"2024-05-26T20:59:17.445454Z"},"trusted":true},"outputs":[],"source":["lr = 1e-3\n","config = {\n","    'output_hidden_states': False,\n","    'num_labels': 2,\n","    'hidden_size': 64,\n","    'final_dropout': 0.1,\n","    'hidden_dropout': 0.2,\n","    'pooling_mode': 'mean',\n","    'model_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian',\n","    'input_size': 1024\n","}\n","\n","model_args_w2v2 = {\n","    \"attention_dropout\": 0.1,\n","    \"hidden_dropout\": 0.1,\n","    \"feat_proj_dropout\": 0.0,\n","    \"mask_time_prob\": 0.05,\n","    \"layerdrop\": 0.1,\n","    'layer_norm_eps':1e-4,\n","}\n","\n","model_args_w2v2_config = Wav2Vec2Config.from_pretrained(\n","    config['model_name'], \n","    attention_dropout=0.1,\n","    hidden_dropout=0.1,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.1,\n","#     layer_norm_eps:1e-4,\n",")\n","\n","model = Wav2VecClassifer(model_args_w2v2_config, config).cuda()\n","model.freeze_feature_extractor()\n","\n","optimizer = torch.optim.AdamW([\n","        {\"params\": model.wav2vec2.parameters(), \"lr\": 3e-4},\n","    ], lr=lr)\n","\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=total_steps * warmup_ratio,\n","    num_training_steps=total_steps\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"/content/model_one_trainer_{lr}\",\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    evaluation_strategy=\"steps\",\n","    num_train_epochs=num_epochs,\n","    fp16=True,\n","    save_steps=3000,\n","    eval_steps=1500,\n","    learning_rate=lr,\n","    save_total_limit=3,\n","    report_to='wandb',\n","    run_name=f\"model_2_{lr}_w2v_3e-4_lstm_one_segment\",\n","    logging_steps=1500,\n","    load_best_model_at_end=True,\n","    seed=888\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train,\n","    eval_dataset=val,\n","    tokenizer=processor,\n","    optimizers=(optimizer, scheduler)\n",")\n","\n","trainer.train()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4494907,"sourceId":8480692,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0de778fff8a64cb089e3ae5bad4ca3e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25d14ebc364145e9a635c394d4f69d33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33da96d697b0497da57d6c51dac2b573","placeholder":"​","style":"IPY_MODEL_e802144479d84603b99e3d6c718cc21a","value":"Loading dataset from disk: 100%"}},"33da96d697b0497da57d6c51dac2b573":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44c1eafe3ea248aa9360f44c6d808d78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4aaba4e709844edaa8926ffe5696675f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"623d9824a7544de8b050730f4ebac305":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"665e535568a1464dbdb76069bf14375f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d98cae5ea863464299cc52aa58f96bf0","placeholder":"​","style":"IPY_MODEL_0de778fff8a64cb089e3ae5bad4ca3e6","value":" 24/24 [00:00&lt;00:00, 53.01it/s]"}},"912ca1284c2c4bfd88b86e13f70394d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_25d14ebc364145e9a635c394d4f69d33","IPY_MODEL_c91d6f89bae849d6a6b6970ccdcd89c3","IPY_MODEL_665e535568a1464dbdb76069bf14375f"],"layout":"IPY_MODEL_4aaba4e709844edaa8926ffe5696675f"}},"c91d6f89bae849d6a6b6970ccdcd89c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_623d9824a7544de8b050730f4ebac305","max":24,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44c1eafe3ea248aa9360f44c6d808d78","value":24}},"d98cae5ea863464299cc52aa58f96bf0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e802144479d84603b99e3d6c718cc21a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
