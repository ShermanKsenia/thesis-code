{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T09:30:25.065295Z","iopub.status.busy":"2024-05-27T09:30:25.064564Z","iopub.status.idle":"2024-05-27T09:30:38.339474Z","shell.execute_reply":"2024-05-27T09:30:38.338192Z","shell.execute_reply.started":"2024-05-27T09:30:25.065263Z"},"id":"mXypXG-ppj9o","trusted":true},"outputs":[],"source":["!pip install transformers[torch] -q"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T09:30:38.342085Z","iopub.status.busy":"2024-05-27T09:30:38.341673Z","iopub.status.idle":"2024-05-27T09:30:52.049109Z","shell.execute_reply":"2024-05-27T09:30:52.047894Z","shell.execute_reply.started":"2024-05-27T09:30:38.342033Z"},"id":"wwIXFj7fwjJp","trusted":true},"outputs":[],"source":["!pip install datasets -q --upgrade"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T09:30:52.050877Z","iopub.status.busy":"2024-05-27T09:30:52.050564Z","iopub.status.idle":"2024-05-27T09:31:04.898094Z","shell.execute_reply":"2024-05-27T09:31:04.896821Z","shell.execute_reply.started":"2024-05-27T09:30:52.050845Z"},"trusted":true},"outputs":[],"source":["!pip install evaluate -q"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T09:31:04.900864Z","iopub.status.busy":"2024-05-27T09:31:04.900526Z","iopub.status.idle":"2024-05-27T09:31:21.109146Z","shell.execute_reply":"2024-05-27T09:31:21.107979Z","shell.execute_reply.started":"2024-05-27T09:31:04.900833Z"},"trusted":true},"outputs":[],"source":["!pip install wandb -q --upgrade"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T09:31:21.111013Z","iopub.status.busy":"2024-05-27T09:31:21.110611Z","iopub.status.idle":"2024-05-27T09:31:41.233547Z","shell.execute_reply":"2024-05-27T09:31:41.232588Z","shell.execute_reply.started":"2024-05-27T09:31:21.110980Z"},"id":"jaSYKqmCs9PP","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-27 09:31:32.963182: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-27 09:31:32.963289: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-27 09:31:33.086182: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import os\n","import gc\n","import math\n","import librosa\n","import numpy as np\n","import pandas as pd\n","import librosa as lb\n","import matplotlib.pyplot as plt\n","\n","from scipy import signal\n","from scipy.fft import fftshift\n","\n","from collections import defaultdict\n","from itertools import islice\n","from typing import Any\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","import torch\n","import torchaudio\n","\n","from IPython import display\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from tqdm.auto import trange\n","from tqdm import tqdm\n","tqdm.pandas()\n","\n","import re\n","import seaborn as sns\n","from dataclasses import dataclass, field\n","from typing import Any, Dict, List, Optional, Union\n","from datasets import (\n","    load_dataset,\n","    load_metric,\n","    load_from_disk,\n","    Audio,\n","    concatenate_datasets\n",")\n","from transformers import (\n","    AutoConfig,\n","    Wav2Vec2Processor,\n","    Wav2Vec2Tokenizer,\n","    Wav2Vec2Config,\n","    Wav2Vec2FeatureExtractor,\n","    Wav2Vec2CTCTokenizer,\n","    Wav2Vec2ForSequenceClassification,\n","    Wav2Vec2Model,\n","    TrainingArguments,\n","    Trainer,\n","    get_linear_schedule_with_warmup,\n","    EvalPrediction\n",")\n","\n","from transformers.models.wav2vec2.modeling_wav2vec2 import (\n","    Wav2Vec2PreTrainedModel,\n","    Wav2Vec2Model\n",")\n","import evaluate"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T09:33:01.257500Z","iopub.status.busy":"2024-05-27T09:33:01.256859Z","iopub.status.idle":"2024-05-27T09:33:01.263776Z","shell.execute_reply":"2024-05-27T09:33:01.262757Z","shell.execute_reply.started":"2024-05-27T09:33:01.257466Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'2.1.2'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["torch.__version__"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T09:33:13.296073Z","iopub.status.busy":"2024-05-27T09:33:13.295419Z","iopub.status.idle":"2024-05-27T09:33:13.301981Z","shell.execute_reply":"2024-05-27T09:33:13.300927Z","shell.execute_reply.started":"2024-05-27T09:33:13.296039Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'4.39.3'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import transformers\n","transformers.__version__"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:13:51.895386Z","iopub.status.busy":"2024-05-23T08:13:51.895080Z","iopub.status.idle":"2024-05-23T08:13:56.451087Z","shell.execute_reply":"2024-05-23T08:13:56.450196Z","shell.execute_reply.started":"2024-05-23T08:13:51.895361Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:13:56.452796Z","iopub.status.busy":"2024-05-23T08:13:56.452175Z","iopub.status.idle":"2024-05-23T08:13:56.457749Z","shell.execute_reply":"2024-05-23T08:13:56.456403Z","shell.execute_reply.started":"2024-05-23T08:13:56.452770Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"WANDB_PROJECT\"]=\"thesis_model_batch\"\n","os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:13:56.460359Z","iopub.status.busy":"2024-05-23T08:13:56.459998Z","iopub.status.idle":"2024-05-23T08:13:56.474608Z","shell.execute_reply":"2024-05-23T08:13:56.473714Z","shell.execute_reply.started":"2024-05-23T08:13:56.460309Z"},"id":"-ftrJdwDN9Cp","outputId":"5fe5fb9e-89c0-43bf-afde-ebba4bab29af","trusted":true},"outputs":[{"data":{"text/plain":["'2.19.1'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import datasets\n","datasets.__version__"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:13:58.739963Z","iopub.status.busy":"2024-05-23T08:13:58.739566Z","iopub.status.idle":"2024-05-23T08:13:58.745197Z","shell.execute_reply":"2024-05-23T08:13:58.744143Z","shell.execute_reply.started":"2024-05-23T08:13:58.739933Z"},"id":"_l3dxXdItFTv","trusted":true},"outputs":[],"source":["train_path = '/kaggle/input/thesis-train-data/train'\n","train1_path = '/kaggle/input/thesis-model-batch-3sec-8/train1'\n","train2_path = '/kaggle/input/thesis-model-batch-3sec-8/train1'\n","\n","val_path = '/kaggle/input/thesis-model-batch-3sec-8/val_2sec_8.pickle'\n","path_to_save = '/kaggle/working/second_model.pth'\n","\n","model_name = 'facebook/wav2vec2-large-xlsr-53'\n","run_name = 'basic_model_one_tahn_russian'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:13:58.916968Z","iopub.status.busy":"2024-05-23T08:13:58.916655Z","iopub.status.idle":"2024-05-23T08:14:25.750266Z","shell.execute_reply":"2024-05-23T08:14:25.749326Z","shell.execute_reply.started":"2024-05-23T08:13:58.916941Z"},"id":"e16LzUvWxJwS","outputId":"f4d0f7e2-e68c-4058-9d4a-10e1fb16bae8","trusted":true},"outputs":[],"source":["train1 = load_from_disk(train1_path)\n","train2 = load_from_disk(train2_path)\n","\n","val_pd = pd.read_pickle(val_path)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:25.752187Z","iopub.status.busy":"2024-05-23T08:14:25.751895Z","iopub.status.idle":"2024-05-23T08:14:25.774693Z","shell.execute_reply":"2024-05-23T08:14:25.773982Z","shell.execute_reply.started":"2024-05-23T08:14:25.752162Z"},"trusted":true},"outputs":[],"source":["from datasets import concatenate_datasets\n","train = concatenate_datasets([train1, train2])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:25.833216Z","iopub.status.busy":"2024-05-23T08:14:25.832899Z","iopub.status.idle":"2024-05-23T08:14:25.841093Z","shell.execute_reply":"2024-05-23T08:14:25.840390Z","shell.execute_reply.started":"2024-05-23T08:14:25.833184Z"},"id":"_nNHoRpExzMy","trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, input_values, labels, audio_paths):\n","        super().__init__()\n","        self.input_values = input_values\n","        self.labels = labels\n","        self.audio_paths = audio_paths\n","\n","    def __len__(self):\n","        return len(self.input_values)\n","\n","    def __getitem__(self, idx):\n","        return {'input_values': self.input_values[idx].tolist(), 'labels': self.labels[idx], 'audio_ID': self.audio_paths[idx]}"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:25.842307Z","iopub.status.busy":"2024-05-23T08:14:25.842037Z","iopub.status.idle":"2024-05-23T08:14:25.853481Z","shell.execute_reply":"2024-05-23T08:14:25.852714Z","shell.execute_reply.started":"2024-05-23T08:14:25.842285Z"},"id":"15NvsGx0y2ZX","trusted":true},"outputs":[],"source":["val = CustomDataset(\n","    input_values = val_pd['input_values'].tolist(),\n","    labels = val_pd['labels'].tolist(),\n","    audio_paths = val_pd['audio_ID'].tolist()\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:25.854626Z","iopub.status.busy":"2024-05-23T08:14:25.854396Z","iopub.status.idle":"2024-05-23T08:14:26.229834Z","shell.execute_reply":"2024-05-23T08:14:26.228652Z","shell.execute_reply.started":"2024-05-23T08:14:25.854605Z"},"id":"TgAK6KgWzR22","trusted":true},"outputs":[{"data":{"text/plain":["1364"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del val_pd\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"VmaIRf_i8h86"},"source":["## Initialize model"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:26.289571Z","iopub.status.busy":"2024-05-23T08:14:26.289024Z","iopub.status.idle":"2024-05-23T08:14:26.298872Z","shell.execute_reply":"2024-05-23T08:14:26.297955Z","shell.execute_reply.started":"2024-05-23T08:14:26.289545Z"},"id":"4-MphDgXDuEz","trusted":true},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import Optional, Tuple\n","import torch\n","from transformers.file_utils import ModelOutput\n","\n","\n","@dataclass\n","class SpeechClassifierOutput(ModelOutput):\n","    loss: Optional[torch.FloatTensor] = None\n","    logits: torch.FloatTensor = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:18:51.700200Z","iopub.status.busy":"2024-05-23T08:18:51.699849Z","iopub.status.idle":"2024-05-23T08:18:51.727279Z","shell.execute_reply":"2024-05-23T08:18:51.726316Z","shell.execute_reply.started":"2024-05-23T08:18:51.700174Z"},"trusted":true},"outputs":[],"source":["# https://github.com/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb\n","import torch\n","from torch import nn\n","from torch.nn import functional as nnf\n","from tqdm import tqdm\n","tqdm.pandas()\n","\n","from transformers import (\n","    Wav2Vec2Model\n",")\n","from transformers.models.wav2vec2.modeling_wav2vec2 import (\n","    Wav2Vec2PreTrainedModel,\n","    Wav2Vec2Model\n",")\n","\n","class Wav2Vec2ClassificationHead(nn.Module):\n","    \"\"\"Head for wav2vec classification task.\"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(1024, 256)\n","        self.mid_proj = nn.Linear(256, config['hidden_size'])\n","        self.hidden_dropout = nn.Dropout(config['hidden_dropout'])\n","        self.final_dropout = nn.Dropout(config['final_dropout'])\n","        self.out_proj = nn.Linear(config['hidden_size'], config['num_labels'])\n","        self.sigmoid = nn.Sigmoid()\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax()\n","\n","    def forward(self, features, **kwargs):\n","        x = self.hidden_dropout(features)\n","        x = self.dense(x)\n","        x = self.relu(x)\n","        x = self.mid_proj(x)\n","        x = self.relu(x)\n","        x = self.final_dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","class Wav2VecClassifer(Wav2Vec2PreTrainedModel):\n","\n","    def __init__(self, w2v_config, config):\n","        super().__init__(w2v_config)\n","\n","        self.num_labels = config['num_labels']\n","        self.pooling_strategy = config['pooling_mode']\n","        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\n","            config['model_name'], \n","            config=w2v_config\n","        )\n","        self.proj_layer = nn.Linear(config['input_size'], 256)\n","        self.classifier = Wav2Vec2ClassificationHead(config)\n","        self.loss = nn.CrossEntropyLoss()\n","        self.post_init()\n","\n","    def freeze_feature_extractor(self):\n","        self.wav2vec2.feature_extractor._freeze_parameters()\n","\n","    def merged_strategy(\n","            self,\n","            hidden_states,\n","            mode=\"mean\"\n","    ):\n","        if mode == \"mean\":\n","            outputs = torch.mean(hidden_states, dim=1)\n","        elif mode == \"sum\":\n","            outputs = torch.sum(hidden_states, dim=1)\n","        elif mode == \"max\":\n","            outputs = torch.max(hidden_states, dim=1)[0]\n","        else:\n","            raise Exception(\n","                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n","\n","        return outputs\n","\n","    def forward(\n","            self,\n","            input_values,\n","            labels,\n","            attention_mask=None,\n","            device=None,\n","            output_attentions=False,\n","            output_hidden_states=False,\n","            number_of_chunks=None\n","    ):\n","\n","        number_of_chunks = [len(input_values)]\n","\n","        outputs = self.wav2vec2(\n","            input_values=input_values,\n","            attention_mask=attention_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states\n","        )\n","\n","        hidden_states = self.merged_strategy(outputs.last_hidden_state, mode='mean')\n","        # # var 1: pooled_over model\n","        logits = self.classifier(hidden_states)\n","        preds = logits.flatten().view(-1, 2)\n","        preds_split = preds.split(number_of_chunks)\n","\n","        if self.pooling_strategy == \"mean\":\n","            pooled_preds = torch.cat([torch.mean(x, dim=0) for x in preds_split])\n","        elif self.pooling_strategy == \"max\":\n","            pooled_preds = torch.cat([torch.max(x).reshape(1).repeat(len(x)) for x in preds_split])\n","\n","        #var 2: batched model\n","        hidden_states = outputs.last_hidden_state\n","        all_hs = []\n","        for hs in hidden_states:\n","            all_hs.append(hs.view(1, -1, 1))\n","        chunks_num = len(all_hs) - 1\n","\n","        concat_states = torch.cat(all_hs, dim=2)\n","        raw = nnf.fold(concat_states, (249+100*chunks_num, 1024), kernel_size=(249, 1024), stride=(100, 1024))\n","        counter = nnf.fold(torch.ones_like(concat_states), (249+100*chunks_num, 1024), kernel_size=(249, 1024), stride=(100, 1024))\n","        result = raw / counter\n","        avg_state = result.squeeze(0).mean(dim=1)\n","        pooled_preds = self.classifier(avg_state)\n","\n","        loss = None\n","        if labels is not None:\n","            loss = self.loss(pooled_preds.view(-1, 2), labels.view(-1))\n","        output = (pooled_preds,) + outputs[2:]\n","        return ((loss,) + output) if loss is not None else output"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:26.327266Z","iopub.status.busy":"2024-05-23T08:14:26.326936Z","iopub.status.idle":"2024-05-23T08:14:32.711941Z","shell.execute_reply":"2024-05-23T08:14:32.711128Z","shell.execute_reply.started":"2024-05-23T08:14:26.327235Z"},"id":"Gk_FY8Rd_SNx","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bea64e4b567a42e6ab19ed8efe36047c","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.78k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9f36904c38c4b049a55b1bb98529df6","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-russian and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["config = {\n","    'output_hidden_states': False,\n","    'num_labels': 2,\n","    'hidden_size': 64,\n","    'final_dropout': 0.1,\n","    'hidden_dropout': 0.2,\n","    'pooling_mode': 'mean',\n","    'model_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian',\n","    'input_size': 1024\n","}\n","\n","model_args_w2v2 = {\n","    \"attention_dropout\": 0.1,\n","    \"hidden_dropout\": 0.1,\n","    \"feat_proj_dropout\": 0.0,\n","    \"mask_time_prob\": 0.05,\n","    \"layerdrop\": 0.1,\n","    'layer_norm_eps':1e-4,\n","}\n","\n","model_args_w2v2_config = Wav2Vec2Config.from_pretrained(\n","    config['model_name'], \n","    attention_dropout=0.1,\n","    hidden_dropout=0.1,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.1,\n","#     layer_norm_eps:1e-4,\n",")\n","\n","model = Wav2VecClassifer(model_args_w2v2_config, config).cuda()"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:32.713404Z","iopub.status.busy":"2024-05-23T08:14:32.713101Z","iopub.status.idle":"2024-05-23T08:14:32.717765Z","shell.execute_reply":"2024-05-23T08:14:32.716801Z","shell.execute_reply.started":"2024-05-23T08:14:32.713379Z"},"trusted":true},"outputs":[],"source":["model.freeze_feature_extractor()"]},{"cell_type":"markdown","metadata":{},"source":["### Trainer"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:32.736625Z","iopub.status.busy":"2024-05-23T08:14:32.736288Z","iopub.status.idle":"2024-05-23T08:14:32.751261Z","shell.execute_reply":"2024-05-23T08:14:32.750256Z","shell.execute_reply.started":"2024-05-23T08:14:32.736594Z"},"trusted":true},"outputs":[],"source":["from torch import Tensor\n","\n","def collate_fn_pooled_tokens(data):\n","    input_ids = [torch.tensor(data[i]['input_values']) for i in range(len(data))]\n","    attention_mask = [torch.where(input_ids[i]==0, 0, 1) for i in range(len(input_ids))]\n","    label_features = [data[i]['labels'] for i in range(len(data))]\n","    d_type = torch.long if isinstance(label_features[0], int) else torch.float\n","    if len(data[0]) == 2:\n","        collated = {\n","            'input_values': input_ids[0],\n","            'attention_mask': attention_mask[0],\n","            'labels': torch.tensor(label_features, dtype=d_type)\n","        }\n","    else:\n","        collated = {\n","            'input_values': input_ids[0],\n","            'attention_mask': attention_mask[0],\n","            'labels': torch.tensor(label_features, dtype=d_type)\n","        }\n","    return collated"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:32.752670Z","iopub.status.busy":"2024-05-23T08:14:32.752405Z","iopub.status.idle":"2024-05-23T08:14:32.961023Z","shell.execute_reply":"2024-05-23T08:14:32.960185Z","shell.execute_reply.started":"2024-05-23T08:14:32.752648Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c63a57406ed44baab867fdc9973fdc23","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["processor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-large-xlsr-53')\n","data_collator = collate_fn_pooled_tokens"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:33.617612Z","iopub.status.busy":"2024-05-23T08:14:33.617324Z","iopub.status.idle":"2024-05-23T08:14:33.625311Z","shell.execute_reply":"2024-05-23T08:14:33.624299Z","shell.execute_reply.started":"2024-05-23T08:14:33.617588Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","    labels = p.label_ids\n","\n","    cr = evaluate.load('bstrai/classification_report')\n","    result = cr.compute(predictions=preds, references=labels, labels=[0, 1])\n","    \n","    res_dict = {}\n","    for label in result.keys():\n","        if isinstance(result[label], dict):\n","            for metric in result[label].keys():\n","                res_dict[f'{metric}_{label}'] = result[label][metric]\n","        else:\n","            res_dict[label] = result[label]\n","\n","    return res_dict"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:33.626970Z","iopub.status.busy":"2024-05-23T08:14:33.626565Z","iopub.status.idle":"2024-05-23T08:14:33.641003Z","shell.execute_reply":"2024-05-23T08:14:33.640229Z","shell.execute_reply.started":"2024-05-23T08:14:33.626935Z"},"trusted":true},"outputs":[],"source":["import random\n","def set_seed(seed: int, deterministic: bool = False):\n","        \"\"\"\n","        Helper function for reproducible behavior to set the seed in `random`, `numpy`, `torch` and/or `tf` (if installed).\n","\n","        Args:\n","            seed (`int`):\n","                The seed to set.\n","            deterministic (`bool`, *optional*, defaults to `False`):\n","                Whether to use deterministic algorithms where available. Can slow down training.\n","        \"\"\"\n","        random.seed(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        # ^^ safe to call this function even if cuda is not available\n","        if deterministic:\n","            torch.use_deterministic_algorithms(True)\n","\n","set_seed(888)"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:33.642223Z","iopub.status.busy":"2024-05-23T08:14:33.641938Z","iopub.status.idle":"2024-05-23T08:14:33.650681Z","shell.execute_reply":"2024-05-23T08:14:33.649664Z","shell.execute_reply.started":"2024-05-23T08:14:33.642200Z"},"trusted":true},"outputs":[],"source":["from transformers import TrainingArguments"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:33.652569Z","iopub.status.busy":"2024-05-23T08:14:33.651808Z","iopub.status.idle":"2024-05-23T08:14:33.661864Z","shell.execute_reply":"2024-05-23T08:14:33.660998Z","shell.execute_reply.started":"2024-05-23T08:14:33.652539Z"},"trusted":true},"outputs":[],"source":["num_epochs = 2\n","warmup_ratio = 0.1\n","total_steps = len(train) * num_epochs"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:14:33.664166Z","iopub.status.busy":"2024-05-23T08:14:33.663256Z","iopub.status.idle":"2024-05-23T08:14:33.673500Z","shell.execute_reply":"2024-05-23T08:14:33.672609Z","shell.execute_reply.started":"2024-05-23T08:14:33.664131Z"},"trusted":true},"outputs":[{"data":{"text/plain":["1128"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["len(val)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T08:18:57.324751Z","iopub.status.busy":"2024-05-23T08:18:57.323941Z","iopub.status.idle":"2024-05-23T15:53:18.874113Z","shell.execute_reply":"2024-05-23T15:53:18.873137Z","shell.execute_reply.started":"2024-05-23T08:18:57.324718Z"},"trusted":true},"outputs":[],"source":["# for lr in [1e-3, 2e-3, 1e-4]\n","lr = 1e-3\n","config = {\n","    'output_hidden_states': False,\n","    'num_labels': 2,\n","    'hidden_size': 64,\n","    'final_dropout': 0.1,\n","    'hidden_dropout': 0.2,\n","    'pooling_mode': 'mean',\n","    'model_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian',\n","    'input_size': 1024\n","}\n","\n","model_args_w2v2 = {\n","    \"attention_dropout\": 0.1,\n","    \"hidden_dropout\": 0.1,\n","    \"feat_proj_dropout\": 0.0,\n","    \"mask_time_prob\": 0.05,\n","    \"layerdrop\": 0.1,\n","    'layer_norm_eps':1e-4,\n","}\n","\n","model_args_w2v2_config = Wav2Vec2Config.from_pretrained(\n","    config['model_name'], \n","    attention_dropout=0.1,\n","    hidden_dropout=0.1,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.1,\n","#     layer_norm_eps:1e-4,\n",")\n","\n","model = Wav2VecClassifer(model_args_w2v2_config, config).cuda()\n","model.freeze_feature_extractor()\n","\n","optimizer = torch.optim.AdamW([\n","        {\"params\": model.wav2vec2.parameters(), \"lr\": 3e-4},\n","    ], lr=lr)\n","\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=total_steps * warmup_ratio,\n","    num_training_steps=total_steps\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"/content/model_one_trainer_{lr}\",\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    evaluation_strategy=\"steps\",\n","    num_train_epochs=num_epochs,\n","    fp16=True,\n","    save_steps=5000,\n","    eval_steps=2500,\n","    learning_rate=lr,\n","    save_total_limit=3,\n","    report_to='wandb',\n","    run_name=f\"model_2_{lr}_w2v_3e-4_pooled_over_more_layers\",\n","    logging_steps=1000,\n","    load_best_model_at_end=True,\n","    seed=888,\n","    save_only_model=True\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train,\n","    eval_dataset=val,\n","    tokenizer=processor,\n","    optimizers=(optimizer, scheduler)\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T15:53:18.876513Z","iopub.status.busy":"2024-05-23T15:53:18.876130Z","iopub.status.idle":"2024-05-23T15:53:34.949218Z","shell.execute_reply":"2024-05-23T15:53:34.948199Z","shell.execute_reply.started":"2024-05-23T15:53:18.876475Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='4821.723 MB of 4821.723 MB uploaded (0.014 MB deduped)\\r'), FloatProgress(value=1.…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁█▇▇█▂▄</td></tr><tr><td>eval/f1-score_0</td><td>█████▁▆</td></tr><tr><td>eval/f1-score_1</td><td>▁▇▇▆▇█▇</td></tr><tr><td>eval/f1-score_macro avg</td><td>▁██▇█▂▆</td></tr><tr><td>eval/f1-score_weighted avg</td><td>▁██▇█▂▆</td></tr><tr><td>eval/loss</td><td>▆▄▃▂▁█▄</td></tr><tr><td>eval/precision_0</td><td>▅██▇█▁▇</td></tr><tr><td>eval/precision_1</td><td>▁▇▇██▆▆</td></tr><tr><td>eval/precision_macro avg</td><td>▁████▄▆</td></tr><tr><td>eval/precision_weighted avg</td><td>▁████▄▆</td></tr><tr><td>eval/recall_0</td><td>█▆▆▇▇▁▄</td></tr><tr><td>eval/recall_1</td><td>▁▅▄▄▅█▆</td></tr><tr><td>eval/recall_macro avg</td><td>▁█▇▇█▁▄</td></tr><tr><td>eval/recall_weighted avg</td><td>▁█▇▇█▂▄</td></tr><tr><td>eval/runtime</td><td>▄▂█▂▁▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▅▇▁▇██▇</td></tr><tr><td>eval/steps_per_second</td><td>▅▇▁▇██▇</td></tr><tr><td>eval/support_0</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>eval/support_1</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>eval/support_macro avg</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>eval/support_weighted avg</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▂▁▄▄▄▄▇▄▅█▅▅▆▆▆▇</td></tr><tr><td>train/learning_rate</td><td>▅██▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>████▇▇▆▅▅▅▅▄▃▂▄▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.55496</td></tr><tr><td>eval/f1-score_0</td><td>0.47158</td></tr><tr><td>eval/f1-score_1</td><td>0.61562</td></tr><tr><td>eval/f1-score_macro avg</td><td>0.5436</td></tr><tr><td>eval/f1-score_weighted avg</td><td>0.54603</td></tr><tr><td>eval/loss</td><td>0.68268</td></tr><tr><td>eval/precision_0</td><td>0.55309</td></tr><tr><td>eval/precision_1</td><td>0.55602</td></tr><tr><td>eval/precision_macro avg</td><td>0.55455</td></tr><tr><td>eval/precision_weighted avg</td><td>0.5546</td></tr><tr><td>eval/recall_0</td><td>0.41101</td></tr><tr><td>eval/recall_1</td><td>0.68954</td></tr><tr><td>eval/recall_macro avg</td><td>0.55027</td></tr><tr><td>eval/recall_weighted avg</td><td>0.55496</td></tr><tr><td>eval/runtime</td><td>482.0285</td></tr><tr><td>eval/samples_per_second</td><td>2.34</td></tr><tr><td>eval/steps_per_second</td><td>2.34</td></tr><tr><td>eval/support_0</td><td>545</td></tr><tr><td>eval/support_1</td><td>583</td></tr><tr><td>eval/support_macro avg</td><td>1128</td></tr><tr><td>eval/support_weighted avg</td><td>1128</td></tr><tr><td>total_flos</td><td>2.169130065387264e+19</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>19168</td></tr><tr><td>train/grad_norm</td><td>81425.9375</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.593</td></tr><tr><td>train_loss</td><td>0.64726</td></tr><tr><td>train_runtime</td><td>27244.2407</td></tr><tr><td>train_samples_per_second</td><td>0.704</td></tr><tr><td>train_steps_per_second</td><td>0.704</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">model_2_0.001_w2v_3e-4_pooled_over_more_layers</strong> at: <a href='https://wandb.ai/kseniasherman/thesis_model_batch/runs/k1iw9fu6' target=\"_blank\">https://wandb.ai/kseniasherman/thesis_model_batch/runs/k1iw9fu6</a><br/> View project at: <a href='https://wandb.ai/kseniasherman/thesis_model_batch' target=\"_blank\">https://wandb.ai/kseniasherman/thesis_model_batch</a><br/>Synced 5 W&B file(s), 0 media file(s), 19 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240523_081435-k1iw9fu6/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:13:36.297366Z","iopub.status.idle":"2024-05-23T08:13:36.297793Z","shell.execute_reply":"2024-05-23T08:13:36.297587Z","shell.execute_reply.started":"2024-05-23T08:13:36.297569Z"},"trusted":true},"outputs":[],"source":["assert False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:13:36.299601Z","iopub.status.idle":"2024-05-23T08:13:36.299932Z","shell.execute_reply":"2024-05-23T08:13:36.299786Z","shell.execute_reply.started":"2024-05-23T08:13:36.299772Z"},"trusted":true},"outputs":[],"source":["# for lr in [1e-3, 2e-3, 1e-4]\n","lr = 2e-3\n","config = {\n","    'output_hidden_states': False,\n","    'num_labels': 2,\n","    'hidden_size': 64,\n","    'final_dropout': 0.1,\n","    'hidden_dropout': 0.2,\n","    'pooling_mode': 'mean',\n","    'model_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian',\n","    'input_size': 1024\n","}\n","\n","model_args_w2v2 = {\n","    \"attention_dropout\": 0.1,\n","    \"hidden_dropout\": 0.1,\n","    \"feat_proj_dropout\": 0.0,\n","    \"mask_time_prob\": 0.05,\n","    \"layerdrop\": 0.1,\n","    'layer_norm_eps':1e-4,\n","}\n","\n","model_args_w2v2_config = Wav2Vec2Config.from_pretrained(\n","    config['model_name'], \n","    attention_dropout=0.1,\n","    hidden_dropout=0.1,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.1,\n","#     layer_norm_eps:1e-4,\n",")\n","\n","model = Wav2VecClassifer(model_args_w2v2_config, config).cuda()\n","model.freeze_feature_extractor()\n","\n","optimizer = torch.optim.AdamW([\n","        {\"params\": model.wav2vec2.parameters(), \"lr\": 1e-5},\n","#         {\"params\": model.classification_head.parameters(), \"lr\": 1e-3}\n","    ], lr=lr)\n","\n","scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=total_steps * warmup_ratio,\n","            num_training_steps=total_steps\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"/content/model_one_trainer_{lr}\",\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","#     gradient_accumulation_steps=2,\n","    evaluation_strategy=\"steps\",\n","    num_train_epochs=2,\n","    fp16=True,\n","    save_steps=5000,\n","    eval_steps=500,\n","    learning_rate=lr,\n","    save_total_limit=3,\n","    report_to='wandb',\n","    run_name=f\"model_2_{lr}_7500\",\n","    logging_steps=1000,\n","    warmup_steps=7500,\n","    warmup_ratio=0.1,\n","    load_best_model_at_end=True,\n","#     label_smoothing_factor=1e-5,\n","    seed=888\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train,\n","    eval_dataset=val,\n","    tokenizer=processor,\n","    optimizers=(optimizer, scheduler)\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:13:36.300948Z","iopub.status.idle":"2024-05-23T08:13:36.301293Z","shell.execute_reply":"2024-05-23T08:13:36.301132Z","shell.execute_reply.started":"2024-05-23T08:13:36.301117Z"},"trusted":true},"outputs":[],"source":["wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:13:36.302590Z","iopub.status.idle":"2024-05-23T08:13:36.302885Z","shell.execute_reply":"2024-05-23T08:13:36.302749Z","shell.execute_reply.started":"2024-05-23T08:13:36.302737Z"},"trusted":true},"outputs":[],"source":["# for lr in [1e-3, 2e-3, 1e-4]\n","lr = 1e-4\n","config = {\n","    'output_hidden_states': False,\n","    'num_labels': 2,\n","    'hidden_size': 64,\n","    'final_dropout': 0.1,\n","    'hidden_dropout': 0.2,\n","    'pooling_mode': 'mean',\n","    'model_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian',\n","    'input_size': 1024\n","}\n","\n","model_args_w2v2 = {\n","    \"attention_dropout\": 0.1,\n","    \"hidden_dropout\": 0.1,\n","    \"feat_proj_dropout\": 0.0,\n","    \"mask_time_prob\": 0.05,\n","    \"layerdrop\": 0.1,\n","    'layer_norm_eps':1e-4,\n","}\n","\n","model_args_w2v2_config = Wav2Vec2Config.from_pretrained(\n","    config['model_name'], \n","    attention_dropout=0.1,\n","    hidden_dropout=0.1,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.1,\n","#     layer_norm_eps:1e-4,\n",")\n","\n","model = Wav2VecClassifer(model_args_w2v2_config, config).cuda()\n","model.freeze_feature_extractor()\n","\n","optimizer = torch.optim.AdamW([\n","        {\"params\": model.wav2vec2.parameters(), \"lr\": 1e-5},\n","    ], lr=lr)\n","\n","scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=total_steps * warmup_ratio,\n","            num_training_steps=total_steps\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"/content/model_one_trainer_{lr}\",\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","#     gradient_accumulation_steps=2,\n","    evaluation_strategy=\"steps\",\n","    num_train_epochs=2,\n","    fp16=True,\n","    save_steps=1000,\n","    eval_steps=500,\n","    learning_rate=lr,\n","    save_total_limit=3,\n","    report_to='wandb',\n","    run_name=f\"model_2_{lr}_7500\",\n","    logging_steps=1000,\n","    warmup_steps=7500,\n","    warmup_ratio=0.1,\n","    load_best_model_at_end=True,\n","#     label_smoothing_factor=1e-5,\n","    seed=888\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train,\n","    eval_dataset=val,\n","    tokenizer=processor,\n","    optimizers=(optimizer, None)\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:13:36.304038Z","iopub.status.idle":"2024-05-23T08:13:36.304364Z","shell.execute_reply":"2024-05-23T08:13:36.304203Z","shell.execute_reply.started":"2024-05-23T08:13:36.304191Z"},"trusted":true},"outputs":[],"source":["assert False"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5016475,"sourceId":8514981,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0de778fff8a64cb089e3ae5bad4ca3e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25d14ebc364145e9a635c394d4f69d33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33da96d697b0497da57d6c51dac2b573","placeholder":"​","style":"IPY_MODEL_e802144479d84603b99e3d6c718cc21a","value":"Loading dataset from disk: 100%"}},"33da96d697b0497da57d6c51dac2b573":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44c1eafe3ea248aa9360f44c6d808d78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4aaba4e709844edaa8926ffe5696675f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"623d9824a7544de8b050730f4ebac305":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"665e535568a1464dbdb76069bf14375f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d98cae5ea863464299cc52aa58f96bf0","placeholder":"​","style":"IPY_MODEL_0de778fff8a64cb089e3ae5bad4ca3e6","value":" 24/24 [00:00&lt;00:00, 53.01it/s]"}},"912ca1284c2c4bfd88b86e13f70394d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_25d14ebc364145e9a635c394d4f69d33","IPY_MODEL_c91d6f89bae849d6a6b6970ccdcd89c3","IPY_MODEL_665e535568a1464dbdb76069bf14375f"],"layout":"IPY_MODEL_4aaba4e709844edaa8926ffe5696675f"}},"c91d6f89bae849d6a6b6970ccdcd89c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_623d9824a7544de8b050730f4ebac305","max":24,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44c1eafe3ea248aa9360f44c6d808d78","value":24}},"d98cae5ea863464299cc52aa58f96bf0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e802144479d84603b99e3d6c718cc21a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
